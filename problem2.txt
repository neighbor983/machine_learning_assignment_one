Multiple Linear Regression

Model Y as a linear function of X1 and X2.  Assume that the learning rate is 
0.01, and initial values of the parameters are [1, 1, 1].  

a.	Illustrate gradient descent algorithm by updating the parameters 3 
iterations.  

alpha = .01;
theta0 = 1;
theta1 = 1;
theta2 = 1;
m = 5;

Equations:
theta0 = theta0 - (alpha / m ) * Sum( ( theta0 + theta1 * x1i + theta2 * x2i ) - yi);
theta1 = theta1 - (alpha / m ) * Sum( ( ( theta0 + theta1 * x1i + theta2 * x2i ) - yi) * x1i);
theta2 = theta2 - (alpha / m ) * Sum( ( ( theta0 + theta1 * x1i + theta2 * x2i ) - yi) * x2i);

After first Iteration:
theta0 = theta0 - (alpha / m ) * Sum( ( theta0 + theta1 * x1i + theta2 * x2i ) - yi)
SUMMATION = (
( ( 1 + 1 * 0 + 1 * 1 ) - 0.6 ) +
( ( 1 + 1 * 1 + 1 * 0 ) - 2.4 ) +
( ( 1 + 1 * 1 + 1 * 1 ) - 1.6 ) +
( ( 1 + 1 * 2 + 1 * 1 ) - 3.4 ) +
( ( 1 + 1 * 1 + 1 * 2 ) - 0.5 )
)
after substituting values becomes: 
theta0 = 1 - ( .01 / 5 ) * 6.5
theta0 = 0.987

theta1 = theta1 - (alpha / m ) * Sum( ( ( theta0 + theta1 * x1i + theta2 * x2i ) - yi) * x1i)
SUMMATION =(
( ( ( 1 + 1 * 0 + 1 * 1 ) - 0.6 ) * 0 ) +
( ( ( 1 + 1 * 1 + 1 * 0 ) - 2.4 ) * 1 ) +
( ( ( 1 + 1 * 1 + 1 * 1 ) - 1.6 ) * 1 ) +
( ( ( 1 + 1 * 2 + 1 * 1 ) - 3.4 ) * 2 ) +
( ( ( 1 + 1 * 1 + 1 * 2 ) - 0.5 ) * 1 ) 
)
after substituting values becomes: 
theta1 = 1 - ( .01 / 5 ) * 5.7
theta1 = 0.9886

theta2 = theta2 - (alpha / m ) * Sum( ( ( theta0 + theta1 * x1i + theta2 * x2i ) - yi) * x2i)
SUMMATION =(
( ( ( 1 + 1 * 0 + 1 * 1 ) - 0.6 ) * 0 ) +
( ( ( 1 + 1 * 1 + 1 * 0 ) - 2.4 ) * 1 ) +
( ( ( 1 + 1 * 1 + 1 * 1 ) - 1.6 ) * 1 ) +
( ( ( 1 + 1 * 2 + 1 * 1 ) - 3.4 ) * 2 ) +
( ( ( 1 + 1 * 1 + 1 * 2 ) - 0.5 ) * 1 ) 
)
after substituting values becomes: 
theta2 = 1 - ( .01 / 5 ) * 10.4
theta2 = 0.9792

After Second Iteration:
theta0 = theta0 - (alpha / m ) * Sum( ( theta0 + theta1 * x1i + theta2 * x2i ) - yi)
SUMMATION = (
( ( ( 0.987 + 0.9886 * 0 + 0.9792 * 1 ) - 0.6 ) * 0 ) +
( ( ( 0.987 + 0.9886 * 1 + 0.9792 * 0 ) - 2.4 ) * 1 ) +
( ( ( 0.987 + 0.9886 * 1 + 0.9792 * 1 ) - 1.6 ) * 1 ) +
( ( ( 0.987 + 0.9886 * 2 + 0.9792 * 1 ) - 3.4 ) * 2 ) +
( ( ( 0.987 + 0.9886 * 1 + 0.9792 * 2 ) - 0.5 ) * 1 )
)
after substituting values becomes: 
theta0 = 0.987 - ( .01 / 5 ) * 6.274
theta0 = 0.974452

theta1 = theta1 - (alpha / m ) * Sum( ( ( theta0 + theta1 * x1i + theta2 * x2i ) - yi) * x1i)
SUMMATION = (
( ( ( 0.987 + 0.9886 * 0 + 0.9792 * 1 ) - 0.6 ) * 0 ) +
( ( ( 0.987 + 0.9886 * 1 + 0.9792 * 0 ) - 2.4 ) * 1 ) +
( ( ( 0.987 + 0.9886 * 1 + 0.9792 * 1 ) - 1.6 ) * 1 ) +
( ( ( 0.987 + 0.9886 * 2 + 0.9792 * 1 ) - 3.4 ) * 2 ) +
( ( ( 0.987 + 0.9886 * 1 + 0.9792 * 2 ) - 0.5 ) * 1 ) 
)
after substituting values becomes: 
theta1 = 0.9886 - ( .01 / 5 ) * 5.4512
theta1 = 0.9776976

theta2 = theta2 - (alpha / m ) * Sum( ( ( theta0 + theta1 * x1i + theta2 * x2i ) - yi) * x2i)
SUMMATION =(
( ( ( 0.987 + 0.9886 * 0 + 0.9792 * 1 ) - 0.6 ) * 1 ) +
( ( ( 0.987 + 0.9886 * 1 + 0.9792 * 0 ) - 2.4 ) * 0 ) +
( ( ( 0.987 + 0.9886 * 1 + 0.9792 * 1 ) - 1.6 ) * 1 ) +
( ( ( 0.987 + 0.9886 * 2 + 0.9792 * 1 ) - 3.4 ) * 1 ) +
( ( ( 0.987 + 0.9886 * 1 + 0.9792 * 2 ) - 0.5 ) * 2 ) 
)
after substituting values becomes: 
theta2 = 0.9792 - ( .01 / 5 ) * 10.1324
theta2 = 0.9589352

After Third Iteration:
theta0 = theta0 - (alpha / m ) * Sum( ( theta0 + theta1 * x1i + theta2 * x2i ) - yi)
SUMMATION = (
( ( 0.974452 + 0.9776976 * 0 + 0.9589352 * 1 ) - 0.6 ) +
( ( 0.974452 + 0.9776976 * 1 + 0.9589352 * 0 ) - 2.4 ) +
( ( 0.974452 + 0.9776976 * 1 + 0.9589352 * 1 ) - 1.6 ) +
( ( 0.974452 + 0.9776976 * 2 + 0.9589352 * 1 ) - 3.4 ) +
( ( 0.974452 + 0.9776976 * 1 + 0.9589352 * 2 ) - 0.5 ) 
)
after substituting values becomes: 
theta0 = 0.974452 - ( .01 / 5 ) * 6.055424
theta0 = 0.962341152

theta1 = theta1 - (alpha / m ) * Sum( ( ( theta0 + theta1 * x1i + theta2 * x2i ) - yi) * x1i)
SUMMATION = (
( ( ( 0.974452 + 0.9776976 * 0 + 0.9589352 * 1 ) - 0.6 ) * 0 ) +
( ( ( 0.974452 + 0.9776976 * 1 + 0.9589352 * 0 ) - 2.4 ) * 1 ) +
( ( ( 0.974452 + 0.9776976 * 1 + 0.9589352 * 1 ) - 1.6 ) * 1 ) +
( ( ( 0.974452 + 0.9776976 * 2 + 0.9589352 * 1 ) - 3.4 ) * 2 ) +
( ( ( 0.974452 + 0.9776976 * 1 + 0.9589352 * 2 ) - 0.5 ) * 1 )
)
after substituting values becomes: 
theta1 = 0.9776976 - ( .01 / 5 ) * 5.2108192
theta1 = 0.9672759616000001

theta2 = theta2 - (alpha / m ) * Sum( ( ( theta0 + theta1 * x1i + theta2 * x2i ) - yi) * x2i)
SUMMATION = (
( ( ( 0.974452 + 0.9776976 * 0 + 0.9589352 * 1 ) - 0.6 ) * 1 ) +
( ( ( 0.974452 + 0.9776976 * 1 + 0.9589352 * 0 ) - 2.4 ) * 0 ) +
( ( ( 0.974452 + 0.9776976 * 1 + 0.9589352 * 1 ) - 1.6 ) * 1 ) +
( ( ( 0.974452 + 0.9776976 * 2 + 0.9589352 * 1 ) - 3.4 ) * 1 ) +
( ( ( 0.974452 + 0.9776976 * 1 + 0.9589352 * 2 ) - 0.5 ) * 2 )
)
after substituting values becomes: 
theta2 = 0.9589352 - ( .01 / 5 ) * 9.8732944
theta2 = 0.9391886112

b.	Code and run the algorithm till convergence.
Final values
theta0: 1.2109560668544477
theta1: 1.4170364204027397
theta2: -0.9327568863018458
J(theta): 0.7907294440296793

1.2109560668544477 + 1.4170364204027397 * x1 - 0.9327568863018458 * x2

Theta2 decreased consistently throughout the runs till it converged at approximately -.93.

Theta0 decreased from 1.0 to approximately .68 then made a steady increase until it converged at approximately 1.2.

Theta1 decreased to approximately .82 then began to increase to approximately 1.47 then began to decrease until it converged around 1.41.
