Simple Linear Regression

a. Model Y as a linear funciton of X.
Y = theta_naught + theta_1 * X
Y = 1.044 + 1.997 * X

b.Use gradient descent learning algorithm to learn model parameters.  Use an 
appropriate learning rate and convergence criterion. Plot J for the learning 
duration.

theta0 = 0.0;
theta1 = 0.0;
alpha = .1;

I started off with an alpha of 1 and it didn't converge. Then I lowered alpha 
and eventually settled with an alpha of .1 

convergence criterion = new_cost > ( old_cost * 1.00001)

The reason I used this criterion was to force the cost to continuously decrease 
with each iteriation and to force a stop if the new cost doesn't lower the cost 
a noticable difference. I could easily see this value changing based on the use 
case if you have something that doesn't need to be as accurate you could 
increase the difference to reduce the number of runs needed. 