Simple Linear Regression

a. Model Y as a linear funciton of X.
Y = theta_naught + theta_1 * X
Y = 1.044 + 1.997 * X

b.Use gradient descent learning algorithm to learn model parameters.  Use an 
appropriate learning rate and convergence criterion. Plot J for the learning 
duration.

theta0 = 0.0;
theta1 = 0.0;
alpha = .1;

I started off with an alpha of 1 and it didn't converge. Then I lowered alpha 
and eventually settled with an alpha of .1 

convergence criterion = new_cost > ( old_cost * 1.00001)

The reason I used this criterion was to force the cost to continuously decrease 
with each iteriation and to force a stop if the new cost doesn't lower the cost 
a noticable difference. I could easily see this value changing based on the use 
case if you have something that doesn't need to be as accurate you could 
increase the difference to reduce the number of runs needed. 

Theta1 continued to bounce around just above 2.0 until about 50 runs where it begins to slowly move to just below 2.

Theta0 bounced around until about 50 runs where it was .6 then it began to continuously increase to approximately 1.04.

Theta1 converged faster and once it steadied out theta0 found a valley in cost and continued to increase until it converged.  
